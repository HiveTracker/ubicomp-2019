%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigchi]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2019}
\acmYear{2019}
\acmDOI{10.1145/1122445.1122456}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[UbiComp'19]{the ACM Conference on Ubiquitous Computing}{Sept 11--13, 2019}{London, UK}
\acmPrice{15.00}
\acmISBN{978-1-4503-9999-9/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{HiveTracker: 3d Positioning for Ubiquitous Embedded Systems}
%\title{HiveTracker: Embedded 3d Positioning, Algorithms and Characterizations}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{C\'edric Honnet}
%\orcid{1234-5678-9012}
\affiliation{%
  \institution{Sorbonne University, ISIR, CNRS}
  \streetaddress{}
  \city{Paris}
  \country{France}}
\email{cedric@honnet.eu}

\author{Gon\c{c}alo Lopes}
\affiliation{%
  \institution{NeuroGEARS Ltd}
  \streetaddress{}
  \city{London}
  \country{United Kingdom}}
\email{g.lopes@neurogears.org}

\begin{teaserfigure}
\centering
\includegraphics[width=1.0\columnwidth]{Figures/banner.jpg}
\caption{Left: the HTC Vive tracker - Right: our HiveTracker miniaturization.}
\label{Fig:Banner}
\end{teaserfigure}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Recent advances in positional tracking systems for virtual and augmented reality have opened up the possibilities for ubiquitous motion capture technology in the consumer market. However, for many applications such as in performance art, athletics, neuroscience, and medicine, these systems remain too bulky, expensive, and limited to tracking a few objects at a time. In this work, we present a small wireless device that takes advantage of existing HTC Vive lighthouse tracking technology to provide affordable, scalable, and highly accurate positional tracking capabilities. This open hardware and open software project contains several elements, and the latest contributions described in this paper include: (1) a characterization of the optical distortions of the lighthouses, (2) a new cross-platform WebBLE interface, and (3) a real-time in-browser visualization. We also introduce the possibility of: (1) a new adaptive calibration using regression to estimate transformation matrices of lighthouses, and (2) an FPGA approach to allow more photodiodes, and to adapt to HTC Vive lighthouses V2. Finally, we show how the new developments reduce setup costs and increase the accessibility of our tracking technology.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10003120.10003121.10003125</concept_id>
<concept_desc>Human-centered computing~Interaction devices</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Human-centered computing~Interaction devices}


%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Indoor Positioning, Open-source, Low Cost, Wireless Device, Virtual Reality, 3D Tracking}


%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
Robust real-time positional tracking systems have been the target of research for decades to support applications in natural user interfaces (NUI), virtual reality (VR) and augmented reality (AR). All of these interface paradigms require an accurate sampling of the spatial environment of a user, and have to be responsive in real-time in order to sustain the illusion of control. Recent developments in motion sensing and tracking technology for VR have finally moved the field of motion capture technology away from expensive custom engineered spaces with multiple cameras. The need to deploy VR at scale in the consumer market has pushed for scalable and cost-effective solutions, mostly through inside-out techniques using either computer vision based simultaneous localization and mapping (SLAM) or laser-based lighthouse approaches.

Because each device is taking care of its own localization, such inside-out solutions are inherently scalable, allowing for a much larger number of devices to coexist in the same space. This has triggered many researchers to start rethinking the entire approach to full-body motion tracking, replacing the passive reflective beads with active inside-out devices. Unfortunately, size and accuracy remain a limitation for such motion capture applications. Inside-out SLAM based computer vision devices rely on cameras and expensive processors, which both make existing controllers more bulky and increase power requirements.


\subsection{Principle}
An alternative solution to inside-out tracking is to use a beacon-based approach, such as the one used in the HTC Vive Lighthouse tracking system. In this system, a lighthouse base station periodically sweeps the space with a laser at a fixed frequency. Photodiodes placed in tracked objects can detect these sweeps and using either information embedded in modulated light at the beam, or synchronization pulses, can reconstruct the exact angle at which the laser hit the object. Positioning information can be accurately recovered from the photodiodes as long as the timing of each laser hit can be sensed fast enough.


\subsection{Previous work}
We explored the design of a prototype\cite{Quinones2018} that takes advantage of a rare dedicated real-time and parallel processing feature in the nRF52 microcontroller made by Nordic Semi to sense the lighthouse light sweeps, without requiring a dedicated FPGA. This allowed us to build an embedded device with up to five photodiodes that is small, accurate, and cheap enough for full-body motion tracking at scale.
\newline \newline
In this paper, we describe the further development of our miniaturized prototype (see Fig. \ref{Fig:Banner}), and the main hardware and software challenges we had to overcome to increase the accessibility and ease of use of the HiveTracker beyond a proof-of-concept.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Contributions}


\subsection{Optical distortion characterization}

As in any incremental development, we started by a simple version to characterize it and improve it accordingly.

\subsubsection{Test jig}
The 1st step was to verify that we can measure a coherent 10 mm translation wherever we are in our working zone.
We used a CNC \footnote{XY-plotter by Makeblock: \url{http://www.makeblock.com}} to sample positions along a
linear path. We moved the CNC on 7 locations, and for each of them, we sampled 30 position data every 10mm.
The HTC Vive positioning being accurate with the HTC recommended setup, we explored the limits of its capabilities.
Instead of making the base stations face toward each other, we made them look in the same direction, as
represented in figure \ref{Fig:characterization_jig}.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{Figures/characterization_jig.jpg}
  \caption{Characterization jig: each numbered square is a position of the CNC.}
  \label{Fig:characterization_jig}
\end{figure}

\subsubsection{Result}
As we can see in the Figure \ref{Fig:characterization_result}, the HTC Vive positioning system is not linear
in this configuration. The distortion observed, shows that 300mm measured near the base stations, is not the
same as if it is measured further. Nevertheless, this distortion can be corrected now that it is characterized,
a distortion map can be estimated, and real positioning can be predicted from it.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{Figures/characterization_result.jpg}
  \caption{Characterization result: each numbered line is supposed to measure 300mm.}
  \label{Fig:characterization_result}
\end{figure}

\subsection{Firmware}

This project is entirely open source, and tries to be as accessible as possible.
to do so, we worked hard to make our firmware\cite{firmwarerepo} arduino compatible.
Coincidentally, the latest Arduino (called "Nano 33 BLE\footnote{Arduino Nano 33 BLE: \url{https://store.arduino.cc/usa/nano-33-ble}}") happens to use the same microcontroller
(nRF52) which allows us to ensure a long term Arduino support.


\subsection{Bluetooth Low Energy communication}

The nRF52 microcontroller provides a proprietary radio communication protocol for high-bandwidth communication, but also a BLE interface. BLE communication is attractive due to its low power requirements and growing acceptance as a standard in web development stacks. In our goal to make setting up and interfacing with HiveTracker devices more accessible, we decided to embed a lightweight BLE protocol in the device firmware, allowing direct pairing from any WebBLE enabled browser.
As mentioned in the previous section, we use tools to make this project accessible.
Up to now WebBLE was the only way, but a simpler approach was recently published by the ITP-NYU community: p5.ble.js\footnote{p5.ble.js by ITP-NYU: \url{https://ItpNyu.github.io/p5ble-website/}}.
This should hopefully help making this project simple to use.


\subsection{3D Reconstruction and triangulation}

For each photodiode laser hit, we can reconstruct the positioning information of the sensor relative to the origin of the base station. However, it is often desirable to position the device relative to a common reference frame. In our previous work \cite{Quinones2018}, we directly used the SteamVR calibration procedure to extract the location of each base station in absolute VR world coordinates. This was useful when integrating with existing VR applications running on the computer, for example to keep the coordinate frame of the device registered with a VR headset.

However, the base station beacons are completely passive and only require a power supply, so it would also be an advantage to allow the calibration of a common reference frame independent of SteamVR, for situations where we are running a motion capture system in full-wireless mode, independently from a computer.


\subsection{Real-time visualization}

To enable rapid-prototyping, visualization and testing of the HiveTracker performance, we developed an in-browser JavaScript interface, visualization and simulation framework using the Three.js library. This cross-platform approach allows visualizing the 3d positions in any device running Chromium or Chrome (see Fig. \ref{Fig:characterization_result}). This interface can be tested, and modified from our repository \cite{WebappRepo}.
One of the imaginable applications can be a Smartphone webapp for augmented reality.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{Figures/webapp.png}
  \caption{Web app showing the 2 base stations (cubes) and the tracker (sphere).}
  \label{Fig:webapp}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Future work}

\subsection{Adaptive calibration}

To calibrate our own reference, we use a HiveTracker device as the origin, with four photodiodes arranged in a known planar configuration (see fig. \ref{Fig:squared_structure}).

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{Figures/squared_structure.jpg}
  \caption{Squared structure used for calibration.}
  \label{Fig:squared_structure}
\end{figure}

For each pair of photodiode hits from the same base station, we recover two angles, from which we can reconstruct the line from the photodiode to the origin of each base station (Figure \ref{Fig:Reconstruction}A). By projecting each line onto a fixed distance plane in front of the beacon, we recover 2D positions of points at the surface of the beacon plane (Figure \ref{Fig:Reconstruction}B). Given we also know, by definition, the 3D coordinates of each of these points on the origin device, we can fully define a set of 2D/3D correspondences ($N = 4$). This is an instance of the well-known Perspective-N-Point problem for planar fiducial markers, for which there are a number of efficient solutions \cite{Lepetit2008,Garrido-Jurado2014}.

\begin{figure}[h]
\centering
\includegraphics[width=1.0\columnwidth]{Figures/reconstruction.pdf}
\caption{Schematic of reconstruction procedure. (A): Relation of polar coordinates to cartesian coordinates, where $\theta$ and $\phi$ are the angles recovered from the device. (B): Projection of angles into fixed distance plane to generate 2D/3D correspondences.}
\label{Fig:Reconstruction}
\end{figure}

After independently computing the pose of the origin device relative to both beacons, we can make use of bundle adjustment methods such as the Levenberg-Marquardt algorithm \cite{Marquardt1963} to further improve the overall estimate of the beacon poses relative to the origin. In principle, we could extend this procedure to a grid of devices to further improve the estimated quality, as in \cite{Garrido-Jurado2014}.


\subsection{Hardware evolutions potential with FPGAs}

Until now, we focused on keeping the hardware simple and affordable, but a new version of the LightHouses came and
brought new features that could be practical (ex: allowing more than 2 base stations to cover bigger range).
This new version introduces a very different architecture, and our nRF52 approach won't work, we need an FPGA.
Some FPGAs can be extremely small and supported by the open source community \footnote{ Open Software FPGZ Toolchain: \url{http://clifford.at/IceStorm} } such as the Lattice iCE40UL-1K. Its 1.4mm x 1.4mm package allows us to stay 
compact, and to support more photodiodes.
Our current version can minimize rotation occlusions with a tetraedric structure (as in fig. \ref{Fig:tetraedric_structure}), but supporting more photodiodes with an FPGA can only help.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{Figures/tetraedric_structure.jpg}
  \caption{Tetraedric structure for less rotation occlusions.}
  \label{Fig:tetraedric_structure}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}

TODO!! \\
TODO!! \\
TODO!! \\
TODO!! \\
TODO!! \\
TODO!! \\


%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
The authors would like to thank Yvonne Jansen and Alexis Polti for their support and expertise offered during the developpement of this project. We would also like to thank Julien Mellet for his early work on reconstruction algorithms, Chinmay Pendharkar for introducing us to WebBLE, Olivia Seow and Chis Davis for their design and media expertise, and NeuroGEARS Ltd. for the financial support as well as the provided help. This project was partially funded by the "Human Computer Interface Prize" of the Hack-A-Day conference. Finally, this work was also partially performed within the Labex SMART (ANR-11-LABX-65), supported by French state funds managed by the ANR under reference ANR-11-IDEX-0004-02.
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{ubicomp-2019}

%%
%% If your work has an appendix, this is the place to put it.
%\appendix

\end{document}
\endinput
%%
%% End of file `sample-sigchi.tex'.
